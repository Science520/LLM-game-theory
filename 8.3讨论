一点到两点跟老师汇报了工作
近期工作：
我：
1.制作法律相关的数据集，扩充关于法律的10个问题（见“法律问题”.docx），符合有争议，较开放，也有具体答案的特点
2.在原先的评分体系上进行修改，建立了LLM间只看答案评分不互相看答案，而让LLM自行迭代出更好答案的模板（见“只看评分模板”.docx）。这个模板已把大多数会遇到的情况考虑进去
3.用新模板，在法律十个问题的基础上进行测试，测试结果（见“LLM法律十个问题评测”.xlsx）体现如下几个方面：
①当第一轮不在social pressure，第二轮进入时，LLM答案更正式，偏向于论述，且言语逻辑性更强，会使用first、second、lastly等关联词，LLM自己对自身的评分会骤升
②当整个情景处于social pressure下，LLM评分提升较缓慢。LLM对自身答案的改进更多在英语用词专业性提高和论述增加，第一轮到第二轮一般会增加一些表达，第二轮到第三轮可能出现难以提升评分的情况。这表明，我们应该再完善评分体系
③Bard易出现为自己打满分的情况，可以采用让它回看自己评分，对自己一、二轮重新打分的方式。可是“回看”思路对结局没有影响，并且也不能作用于过程，改变过程回答答案的social pressure，那如何避免打满分的情况呢？
④在不更新答案的情况下，让LLM看了比自己分低的答案，让它再自评时，它会降低自己得分
⑤LLM更同意人类对游戏输赢的判决，即使它看自己的分数高于我说对方model的分数
⑥让LLM在一些标准下评分，最后我自行运算最终评分的必要性：LLM认为在某一模块分越高，代表能力越厉害（见“science_count_finalscore”文件夹）
⑦GPT4.0在第一轮犯错，第二轮答对时，第二轮评分会为自己降低第二层次（分析问题）评分，甚至会降低第四层次（上下文理解）的评分。这表明，LLM自评的有效性
⑧在对方分数不过于离谱的情况下，LLM在而三轮会尽量使自己的分数高于对方分数
⑨游戏结束后，让LLM猜测对方得分，GPT4.0对对方每一模块得分很准，当然这也有可能是因为Bard对自己打分较高，每一模块组合得分可能性少的原因（见“只看评分模板”.docx）

Wsx：
1.制作摘要复述型相关的数据集，扩充关于摘要的10个问题（见wsx分支）
2.建立人为评分的新模板，模板实现内容：①让LLM明确任务、额外要求和要概括的文章，以此生成一个答案②人为评分③挨个告诉他们另外两个大模型的评分和答案，让它们给出建议③把给出的建议汇总发给对应的大模型，让它们重新生成答案
如此迭代。最后发现迭代超过四轮，建议重复度较高，可设置迭代三轮
3.这种方式，可让大语言模型明白任务以及我对字数和内容的额外要求。实验结果发现LLM会更多地采纳评分比自己高的大语言模型的建议，不然就会出现本来评分很高的大模型采纳了错误的建议导致评分降低的情况


后续任务：
1.通过组合互相交换信息，评分，答案这三个要素，控制变量观察结果
2.是否在social pressure下，对答案生成的影响：设置第一轮不在压力，第二轮在压力下。或第三轮有压力+知道对方得分，比自己评分高低的影响（虚构评分）
